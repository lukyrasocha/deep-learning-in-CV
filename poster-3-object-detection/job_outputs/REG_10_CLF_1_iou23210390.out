ðŸ”§ WORKING ON: Loading Train data
ðŸ”§ WORKING ON: Loading Validation data
ðŸ”§ WORKING ON: Loading Test data
â„¹ INFO: Using the entire dataset for training and validation.
ðŸ”§ WORKING ON: Training model
â„¹ INFO: Epoch 1/50 - Train Loss: 0.8138 (Cls: 0.3459, Reg: 0.4679) - Val Loss: 
0.4549 (Cls: 0.1153, Reg: 0.3396)
â„¹ INFO: Epoch 2/50 - Train Loss: 0.6402 (Cls: 0.2703, Reg: 0.3699) - Val Loss: 
0.4409 (Cls: 0.1195, Reg: 0.3214)
â„¹ INFO: Epoch 3/50 - Train Loss: 0.5903 (Cls: 0.2430, Reg: 0.3473) - Val Loss: 
0.4277 (Cls: 0.1097, Reg: 0.3180)
â„¹ INFO: Epoch 4/50 - Train Loss: 0.5538 (Cls: 0.2203, Reg: 0.3335) - Val Loss: 
0.4180 (Cls: 0.1040, Reg: 0.3140)
â„¹ INFO: Epoch 5/50 - Train Loss: 0.5231 (Cls: 0.2025, Reg: 0.3206) - Val Loss: 
0.4162 (Cls: 0.1118, Reg: 0.3045)
â„¹ INFO: Epoch 6/50 - Train Loss: 0.4972 (Cls: 0.1916, Reg: 0.3056) - Val Loss: 
0.4196 (Cls: 0.1151, Reg: 0.3045)
â„¹ INFO: Epoch 7/50 - Train Loss: 0.4793 (Cls: 0.1789, Reg: 0.3005) - Val Loss: 
0.3976 (Cls: 0.0947, Reg: 0.3029)
â„¹ INFO: Epoch 8/50 - Train Loss: 0.4534 (Cls: 0.1649, Reg: 0.2885) - Val Loss: 
0.4200 (Cls: 0.1072, Reg: 0.3128)
â„¹ INFO: Epoch 9/50 - Train Loss: 0.4329 (Cls: 0.1532, Reg: 0.2797) - Val Loss: 
0.4054 (Cls: 0.1038, Reg: 0.3017)
â„¹ INFO: Epoch 10/50 - Train Loss: 0.4014 (Cls: 0.1372, Reg: 0.2642) - Val Loss: 
0.4231 (Cls: 0.0962, Reg: 0.3269)
â„¹ INFO: Epoch 11/50 - Train Loss: 0.3873 (Cls: 0.1295, Reg: 0.2578) - Val Loss: 
0.4313 (Cls: 0.1332, Reg: 0.2981)
â„¹ INFO: Epoch 12/50 - Train Loss: 0.3707 (Cls: 0.1226, Reg: 0.2482) - Val Loss: 
0.3925 (Cls: 0.1003, Reg: 0.2922)
â„¹ INFO: Epoch 13/50 - Train Loss: 0.3582 (Cls: 0.1154, Reg: 0.2428) - Val Loss: 
0.4071 (Cls: 0.1205, Reg: 0.2866)
â„¹ INFO: Epoch 14/50 - Train Loss: 0.3363 (Cls: 0.1029, Reg: 0.2334) - Val Loss: 
0.4052 (Cls: 0.1091, Reg: 0.2961)
â„¹ INFO: Epoch 15/50 - Train Loss: 0.3305 (Cls: 0.0962, Reg: 0.2342) - Val Loss: 
0.4351 (Cls: 0.1435, Reg: 0.2915)
â„¹ INFO: Epoch 16/50 - Train Loss: 0.3166 (Cls: 0.0912, Reg: 0.2254) - Val Loss: 
0.4133 (Cls: 0.1166, Reg: 0.2967)
â„¹ INFO: Epoch 17/50 - Train Loss: 0.3052 (Cls: 0.0890, Reg: 0.2163) - Val Loss: 
0.4349 (Cls: 0.1238, Reg: 0.3112)
â„¹ INFO: Epoch 18/50 - Train Loss: 0.2916 (Cls: 0.0781, Reg: 0.2135) - Val Loss: 
0.4266 (Cls: 0.1169, Reg: 0.3097)
â„¹ INFO: Epoch 19/50 - Train Loss: 0.2879 (Cls: 0.0795, Reg: 0.2084) - Val Loss: 
0.4274 (Cls: 0.1291, Reg: 0.2982)
â„¹ INFO: Epoch 20/50 - Train Loss: 0.2790 (Cls: 0.0731, Reg: 0.2060) - Val Loss: 
0.4253 (Cls: 0.1321, Reg: 0.2932)
â„¹ INFO: Epoch 21/50 - Train Loss: 0.2708 (Cls: 0.0686, Reg: 0.2022) - Val Loss: 
0.4129 (Cls: 0.1259, Reg: 0.2870)
â„¹ INFO: Epoch 22/50 - Train Loss: 0.2608 (Cls: 0.0643, Reg: 0.1965) - Val Loss: 
0.4389 (Cls: 0.1516, Reg: 0.2874)
â„¹ INFO: Early stopping triggered.
ðŸ”§ WORKING ON: Visualizing predictions
Image ID: 565
Number of Proposals: 500
Predictions of potholes before NMS: 10
Predictions of potholes after NMS: 1
Image ID: 498
Number of Proposals: 500
Predictions of potholes before NMS: 0
Predictions of potholes after NMS: 0
Image ID: 11
Number of Proposals: 500
Predictions of potholes before NMS: 9
Predictions of potholes after NMS: 3
Image ID: 143
Number of Proposals: 500
Predictions of potholes before NMS: 0
Predictions of potholes after NMS: 0
Image ID: 65
Number of Proposals: 500
Predictions of potholes before NMS: 0
Predictions of potholes after NMS: 0
Image ID: 641
Number of Proposals: 500
Predictions of potholes before NMS: 0
Predictions of potholes after NMS: 0
Image ID: 627
Number of Proposals: 500
Predictions of potholes before NMS: 26
Predictions of potholes after NMS: 6
Image ID: 248
Number of Proposals: 500
Predictions of potholes before NMS: 0
Predictions of potholes after NMS: 0
Image ID: 611
Number of Proposals: 500
Predictions of potholes before NMS: 28
Predictions of potholes after NMS: 5
Image ID: 114
Number of Proposals: 500
Predictions of potholes before NMS: 8
Predictions of potholes after NMS: 2
Image Index: [196]
Number of Proposals: 12
Ground Truth Boxes: 3
Predictions Before NMS: 0
Predictions After NMS: 0
Image Index: [354]
Number of Proposals: 20
Ground Truth Boxes: 5
Predictions Before NMS: 3
Predictions After NMS: 1
Image Index: [77]
Number of Proposals: 24
Ground Truth Boxes: 6
Predictions Before NMS: 0
Predictions After NMS: 0
Image Index: [392]
Number of Proposals: 28
Ground Truth Boxes: 7
Predictions Before NMS: 0
Predictions After NMS: 0
Image Index: [88]
Number of Proposals: 40
Ground Truth Boxes: 10
Predictions Before NMS: 0
Predictions After NMS: 0
ðŸ”§ WORKING ON: Saving model
â„¹ INFO: Model state_dict saved to 
saved_models/model_state_dict_EXPERIMENT8Reg10IoU.pth
â„¹ INFO: Entire model saved to saved_models/model_full_EXPERIMENT8Reg10IoU.pth
ðŸ”§ WORKING ON: Evaluating model on Validation split
â„¹ INFO: Precision-Recall curve saved as 
figures/png/EXPERIMENT8Reg10IoU/precision_recall_curve_val_EXPERIMENT8Reg10IoU_2
0241119-175816.png and 
figures/svg/EXPERIMENT8Reg10IoU/precision_recall_curve_val_EXPERIMENT8Reg10IoU_2
0241119-175816.svg
precision: [1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         0.90909091 0.91666667
 0.92307692 0.92857143 0.93333333 0.875      0.88235294 0.88888889
 0.89473684 0.9        0.85714286 0.81818182 0.7826087  0.79166667
 0.8        0.80769231 0.81481481 0.82142857 0.82758621 0.83333333
 0.83870968 0.84375    0.84848485 0.85294118 0.82857143 0.83333333
 0.81081081 0.81578947 0.82051282 0.825      0.80487805 0.80952381
 0.81395349 0.79545455 0.8        0.7826087  0.78723404 0.79166667
 0.7755102  0.78       0.78431373 0.76923077 0.77358491 0.75925926
 0.76363636 0.76785714 0.77192982 0.75862069 0.76271186 0.75
 0.73770492 0.72580645 0.73015873 0.734375   0.72307692 0.72727273
 0.73134328 0.72058824 0.72463768 0.72857143 0.73239437 0.72222222
 0.7260274  0.72972973 0.73333333 0.72368421 0.72727273 0.71794872
 0.70886076 0.7        0.69135802 0.68292683 0.68674699 0.67857143
 0.67058824 0.6744186  0.67816092 0.67045455 0.66292135 0.66666667
 0.67032967 0.66304348 0.66666667 0.65957447 0.65263158 0.64583333
 0.63917526 0.64285714 0.63636364 0.64       0.64356436 0.64705882
 0.6407767  0.64423077 0.64761905 0.64150943 0.63551402 0.63888889
 0.63302752 0.63636364 0.63063063 0.625      0.61946903 0.61403509
 0.6173913  0.62068966 0.61538462 0.61016949 0.60504202 0.6
 0.59504132 0.59016393 0.59349593 0.59677419 0.592      0.5952381
 0.59055118 0.5859375  0.58139535 0.58461538 0.58778626 0.59090909
 0.58646617 0.58955224 0.59259259 0.59558824 0.59854015 0.5942029
 0.5971223  0.6        0.60283688 0.6056338  0.6013986  0.60416667
 0.60689655 0.60273973 0.60544218 0.60135135 0.59731544 0.59333333
 0.58940397 0.58552632 0.58823529 0.58441558 0.58709677 0.58333333
 0.58598726 0.58860759 0.59119497 0.5875     0.59006211 0.59259259
 0.58895706 0.58536585 0.58787879 0.58433735 0.58083832 0.57738095
 0.57988166 0.58235294 0.57894737 0.58139535 0.57803468 0.57471264
 0.57142857 0.56818182 0.57062147 0.56741573 0.5698324  0.56666667
 0.56353591 0.56043956 0.55737705 0.55434783 0.55675676 0.55376344
 0.55614973 0.55851064 0.55555556 0.55263158 0.54973822 0.55208333
 0.5492228  0.54639175 0.54358974 0.54591837 0.54314721 0.54040404
 0.53768844 0.535      0.53731343 0.53465347 0.5320197  0.52941176
 0.52682927 0.52427184 0.52173913 0.52403846 0.5215311  0.51904762
 0.51658768 0.51415094 0.51173709 0.51401869 0.51627907 0.51388889
 0.51152074 0.50917431 0.51141553 0.51363636 0.51131222 0.51351351
 0.51569507 0.51339286 0.51555556 0.51769912 0.5154185  0.51754386
 0.51528384 0.51304348 0.51082251 0.50862069 0.50643777 0.5042735
 0.50638298 0.50847458 0.50632911 0.50420168 0.50209205 0.5
 0.49792531 0.49586777 0.49382716 0.49590164 0.49795918 0.5
 0.49797571 0.49596774 0.49799197 0.496      0.4940239  0.49206349
 0.49407115 0.49212598 0.49019608 0.4921875  0.49027237 0.48837209
 0.48648649 0.48461538 0.48659004 0.48473282 0.48288973 0.48484848
 0.48679245 0.48496241 0.48689139 0.48507463 0.48327138 0.48518519
 0.48339483 0.48161765 0.48351648 0.48175182 0.48363636 0.48550725
 0.48736462 0.48561151 0.48387097 0.48214286 0.48042705 0.4822695
 0.48409894 0.48591549 0.48421053 0.48251748 0.48083624 0.47916667
 0.48096886 0.47931034 0.48109966 0.47945205 0.48122867 0.4829932
 0.48135593 0.48310811 0.48148148 0.47986577 0.47826087 0.48
 0.47840532 0.47682119 0.47524752 0.47368421 0.47213115 0.47058824
 0.4723127  0.47077922 0.47249191 0.47419355 0.47588424 0.47435897
 0.47284345 0.47452229 0.47301587 0.47151899 0.47003155 0.47169811
 0.47335423 0.471875   0.47040498 0.4689441  0.46749226 0.46604938
 0.46769231 0.46932515 0.46788991 0.46646341 0.46504559 0.46363636
 0.46223565 0.46084337 0.46246246 0.46107784 0.45970149 0.45833333
 0.45994065 0.46153846 0.46312684 0.46176471 0.46041056]
recall: [0.00636943 0.01273885 0.01910828 0.02547771 0.03184713 0.03821656
 0.04458599 0.05095541 0.05732484 0.06369427 0.06369427 0.07006369
 0.07643312 0.08280255 0.08917197 0.08917197 0.0955414  0.10191083
 0.10828025 0.11464968 0.11464968 0.11464968 0.11464968 0.12101911
 0.12738854 0.13375796 0.14012739 0.14649682 0.15286624 0.15923567
 0.1656051  0.17197452 0.17834395 0.18471338 0.18471338 0.1910828
 0.1910828  0.19745223 0.20382166 0.21019108 0.21019108 0.21656051
 0.22292994 0.22292994 0.22929936 0.22929936 0.23566879 0.24203822
 0.24203822 0.24840764 0.25477707 0.25477707 0.2611465  0.2611465
 0.26751592 0.27388535 0.28025478 0.28025478 0.2866242  0.2866242
 0.2866242  0.2866242  0.29299363 0.29936306 0.29936306 0.30573248
 0.31210191 0.31210191 0.31847134 0.32484076 0.33121019 0.33121019
 0.33757962 0.34394904 0.35031847 0.35031847 0.3566879  0.3566879
 0.3566879  0.3566879  0.3566879  0.3566879  0.36305732 0.36305732
 0.36305732 0.36942675 0.37579618 0.37579618 0.37579618 0.38216561
 0.38853503 0.38853503 0.39490446 0.39490446 0.39490446 0.39490446
 0.39490446 0.40127389 0.40127389 0.40764331 0.41401274 0.42038217
 0.42038217 0.42675159 0.43312102 0.43312102 0.43312102 0.43949045
 0.43949045 0.44585987 0.44585987 0.44585987 0.44585987 0.44585987
 0.4522293  0.45859873 0.45859873 0.45859873 0.45859873 0.45859873
 0.45859873 0.45859873 0.46496815 0.47133758 0.47133758 0.47770701
 0.47770701 0.47770701 0.47770701 0.48407643 0.49044586 0.49681529
 0.49681529 0.50318471 0.50955414 0.51592357 0.52229299 0.52229299
 0.52866242 0.53503185 0.54140127 0.5477707  0.5477707  0.55414013
 0.56050955 0.56050955 0.56687898 0.56687898 0.56687898 0.56687898
 0.56687898 0.56687898 0.57324841 0.57324841 0.57961783 0.57961783
 0.58598726 0.59235669 0.59872611 0.59872611 0.60509554 0.61146497
 0.61146497 0.61146497 0.61783439 0.61783439 0.61783439 0.61783439
 0.62420382 0.63057325 0.63057325 0.63694268 0.63694268 0.63694268
 0.63694268 0.63694268 0.6433121  0.6433121  0.64968153 0.64968153
 0.64968153 0.64968153 0.64968153 0.64968153 0.65605096 0.65605096
 0.66242038 0.66878981 0.66878981 0.66878981 0.66878981 0.67515924
 0.67515924 0.67515924 0.67515924 0.68152866 0.68152866 0.68152866
 0.68152866 0.68152866 0.68789809 0.68789809 0.68789809 0.68789809
 0.68789809 0.68789809 0.68789809 0.69426752 0.69426752 0.69426752
 0.69426752 0.69426752 0.69426752 0.70063694 0.70700637 0.70700637
 0.70700637 0.70700637 0.7133758  0.71974522 0.71974522 0.72611465
 0.73248408 0.73248408 0.7388535  0.74522293 0.74522293 0.75159236
 0.75159236 0.75159236 0.75159236 0.75159236 0.75159236 0.75159236
 0.75796178 0.76433121 0.76433121 0.76433121 0.76433121 0.76433121
 0.76433121 0.76433121 0.76433121 0.77070064 0.77707006 0.78343949
 0.78343949 0.78343949 0.78980892 0.78980892 0.78980892 0.78980892
 0.79617834 0.79617834 0.79617834 0.80254777 0.80254777 0.80254777
 0.80254777 0.80254777 0.8089172  0.8089172  0.8089172  0.81528662
 0.82165605 0.82165605 0.82802548 0.82802548 0.82802548 0.8343949
 0.8343949  0.8343949  0.84076433 0.84076433 0.84713376 0.85350318
 0.85987261 0.85987261 0.85987261 0.85987261 0.85987261 0.86624204
 0.87261146 0.87898089 0.87898089 0.87898089 0.87898089 0.87898089
 0.88535032 0.88535032 0.89171975 0.89171975 0.89808917 0.9044586
 0.9044586  0.91082803 0.91082803 0.91082803 0.91082803 0.91719745
 0.91719745 0.91719745 0.91719745 0.91719745 0.91719745 0.91719745
 0.92356688 0.92356688 0.92993631 0.93630573 0.94267516 0.94267516
 0.94267516 0.94904459 0.94904459 0.94904459 0.94904459 0.95541401
 0.96178344 0.96178344 0.96178344 0.96178344 0.96178344 0.96178344
 0.96815287 0.97452229 0.97452229 0.97452229 0.97452229 0.97452229
 0.97452229 0.97452229 0.98089172 0.98089172 0.98089172 0.98089172
 0.98726115 0.99363057 1.         1.         1.        ]
â„¹ INFO: Experiment EXPERIMENT8Reg10IoU - mAP: 0.6577
ðŸ”§ WORKING ON: Evaluating model on Test split
â„¹ INFO: Precision-Recall curve saved as 
figures/png/EXPERIMENT8Reg10IoU/precision_recall_curve_test_EXPERIMENT8Reg10IoU_
20241119-180026.png and 
figures/svg/EXPERIMENT8Reg10IoU/precision_recall_curve_test_EXPERIMENT8Reg10IoU_
20241119-180026.svg
precision: [1.         1.         1.         1.         1.         1.
 1.         1.         0.88888889 0.9        0.90909091 0.91666667
 0.92307692 0.92857143 0.93333333 0.9375     0.88235294 0.88888889
 0.89473684 0.85       0.85714286 0.86363636 0.82608696 0.83333333
 0.8        0.80769231 0.81481481 0.82142857 0.82758621 0.83333333
 0.80645161 0.78125    0.78787879 0.79411765 0.8        0.80555556
 0.81081081 0.81578947 0.82051282 0.825      0.82926829 0.80952381
 0.79069767 0.79545455 0.8        0.80434783 0.78723404 0.79166667
 0.7755102  0.78       0.78431373 0.78846154 0.79245283 0.7962963
 0.8        0.80357143 0.80701754 0.81034483 0.81355932 0.8
 0.80327869 0.80645161 0.80952381 0.8125     0.81538462 0.81818182
 0.82089552 0.80882353 0.8115942  0.8        0.8028169  0.80555556
 0.80821918 0.7972973  0.8        0.78947368 0.79220779 0.79487179
 0.78481013 0.7875     0.77777778 0.7804878  0.77108434 0.76190476
 0.76470588 0.76744186 0.77011494 0.76136364 0.76404494 0.76666667
 0.76923077 0.77173913 0.76344086 0.76595745 0.75789474 0.76041667
 0.75257732 0.75510204 0.75757576 0.76       0.76237624 0.75490196
 0.75728155 0.75961538 0.76190476 0.76415094 0.76635514 0.75925926
 0.75229358 0.74545455 0.74774775 0.75       0.75221239 0.75438596
 0.74782609 0.74137931 0.73504274 0.72881356 0.72268908 0.725
 0.71900826 0.72131148 0.71544715 0.70967742 0.704      0.70634921
 0.70866142 0.703125   0.69767442 0.7        0.70229008 0.6969697
 0.69172932 0.68656716 0.68888889 0.69117647 0.69343066 0.69565217
 0.69784173 0.7        0.70212766 0.6971831  0.6993007  0.69444444
 0.68965517 0.68493151 0.68027211 0.68243243 0.67785235 0.68
 0.67549669 0.67105263 0.67320261 0.66883117 0.66451613 0.66025641
 0.65605096 0.65189873 0.64779874 0.64375    0.63975155 0.63580247
 0.63803681 0.6402439  0.63636364 0.63253012 0.63473054 0.63690476
 0.63313609 0.63529412 0.63157895 0.62790698 0.6300578  0.63218391
 0.62857143 0.63068182 0.63276836 0.63483146 0.63128492 0.63333333
 0.62983425 0.62637363 0.6284153  0.63043478 0.62702703 0.62365591
 0.62566845 0.62234043 0.61904762 0.62105263 0.62303665 0.625
 0.62176166 0.6185567  0.61538462 0.61734694 0.6142132  0.61616162
 0.61306533 0.61       0.60696517 0.6039604  0.60098522 0.59803922
 0.6        0.60194175 0.60386473 0.60096154 0.60287081 0.6
 0.5971564  0.5990566  0.60093897 0.59813084 0.6        0.60185185
 0.59907834 0.59633028 0.59360731 0.59545455 0.59728507 0.59459459
 0.59192825 0.58928571 0.58666667 0.58849558 0.58590308 0.58333333
 0.58078603 0.57826087 0.57575758 0.57758621 0.5751073  0.57264957
 0.57021277 0.56779661 0.56540084 0.56722689 0.56903766 0.56666667
 0.56431535 0.5661157  0.56378601 0.56147541 0.55918367 0.55691057
 0.55870445 0.56048387 0.55823293 0.556      0.55776892 0.55952381
 0.55731225 0.55511811 0.55294118 0.5546875  0.55252918 0.5503876
 0.54826255 0.54615385 0.54789272 0.54961832 0.5513308  0.5530303
 0.5509434  0.54887218 0.5505618  0.54850746 0.55018587 0.54814815
 0.54612546 0.54411765 0.54212454 0.54379562 0.54545455 0.54710145
 0.54512635 0.54316547 0.54121864 0.53928571 0.53736655 0.53546099
 0.5335689  0.53169014 0.52982456 0.52797203 0.5261324  0.52430556
 0.52595156 0.52413793 0.52233677 0.52054795 0.51877133 0.5170068
 0.51864407 0.52027027 0.51851852 0.51677852 0.51505017 0.51333333
 0.51495017 0.51324503 0.51155116 0.51315789 0.51147541 0.50980392
 0.50814332 0.50974026 0.50809061 0.50967742 0.50803859 0.50641026
 0.50479233 0.50318471 0.5015873  0.50316456 0.50157729 0.5
 0.5015674  0.5        0.49844237 0.49689441 0.49535604 0.49382716
 0.49538462 0.49693252 0.49541284 0.49390244 0.49240122 0.49090909
 0.49244713 0.4939759  0.4954955  0.49700599 0.49850746 0.5
 0.49851632 0.5        0.49852507 0.5        0.49853372 0.49707602
 0.49562682 0.49418605 0.49275362 0.49132948 0.48991354 0.48850575
 0.48710602 0.48571429 0.48717949 0.48579545 0.48441926 0.48587571
 0.48450704 0.48314607 0.48179272 0.48044693 0.47910864 0.48055556
 0.48199446 0.48066298 0.48209366 0.48351648 0.48219178 0.48360656
 0.48228883 0.48097826 0.4796748  0.47837838 0.47978437]
recall: [0.00561798 0.01123596 0.01685393 0.02247191 0.02808989 0.03370787
 0.03932584 0.04494382 0.04494382 0.0505618  0.05617978 0.06179775
 0.06741573 0.07303371 0.07865169 0.08426966 0.08426966 0.08988764
 0.09550562 0.09550562 0.1011236  0.10674157 0.10674157 0.11235955
 0.11235955 0.11797753 0.12359551 0.12921348 0.13483146 0.14044944
 0.14044944 0.14044944 0.14606742 0.15168539 0.15730337 0.16292135
 0.16853933 0.1741573  0.17977528 0.18539326 0.19101124 0.19101124
 0.19101124 0.19662921 0.20224719 0.20786517 0.20786517 0.21348315
 0.21348315 0.21910112 0.2247191  0.23033708 0.23595506 0.24157303
 0.24719101 0.25280899 0.25842697 0.26404494 0.26966292 0.26966292
 0.2752809  0.28089888 0.28651685 0.29213483 0.29775281 0.30337079
 0.30898876 0.30898876 0.31460674 0.31460674 0.32022472 0.3258427
 0.33146067 0.33146067 0.33707865 0.33707865 0.34269663 0.34831461
 0.34831461 0.35393258 0.35393258 0.35955056 0.35955056 0.35955056
 0.36516854 0.37078652 0.37640449 0.37640449 0.38202247 0.38764045
 0.39325843 0.3988764  0.3988764  0.40449438 0.40449438 0.41011236
 0.41011236 0.41573034 0.42134831 0.42696629 0.43258427 0.43258427
 0.43820225 0.44382022 0.4494382  0.45505618 0.46067416 0.46067416
 0.46067416 0.46067416 0.46629213 0.47191011 0.47752809 0.48314607
 0.48314607 0.48314607 0.48314607 0.48314607 0.48314607 0.48876404
 0.48876404 0.49438202 0.49438202 0.49438202 0.49438202 0.5
 0.50561798 0.50561798 0.50561798 0.51123596 0.51685393 0.51685393
 0.51685393 0.51685393 0.52247191 0.52808989 0.53370787 0.53932584
 0.54494382 0.5505618  0.55617978 0.55617978 0.56179775 0.56179775
 0.56179775 0.56179775 0.56179775 0.56741573 0.56741573 0.57303371
 0.57303371 0.57303371 0.57865169 0.57865169 0.57865169 0.57865169
 0.57865169 0.57865169 0.57865169 0.57865169 0.57865169 0.57865169
 0.58426966 0.58988764 0.58988764 0.58988764 0.59550562 0.6011236
 0.6011236  0.60674157 0.60674157 0.60674157 0.61235955 0.61797753
 0.61797753 0.62359551 0.62921348 0.63483146 0.63483146 0.64044944
 0.64044944 0.64044944 0.64606742 0.65168539 0.65168539 0.65168539
 0.65730337 0.65730337 0.65730337 0.66292135 0.66853933 0.6741573
 0.6741573  0.6741573  0.6741573  0.67977528 0.67977528 0.68539326
 0.68539326 0.68539326 0.68539326 0.68539326 0.68539326 0.68539326
 0.69101124 0.69662921 0.70224719 0.70224719 0.70786517 0.70786517
 0.70786517 0.71348315 0.71910112 0.71910112 0.7247191  0.73033708
 0.73033708 0.73033708 0.73033708 0.73595506 0.74157303 0.74157303
 0.74157303 0.74157303 0.74157303 0.74719101 0.74719101 0.74719101
 0.74719101 0.74719101 0.74719101 0.75280899 0.75280899 0.75280899
 0.75280899 0.75280899 0.75280899 0.75842697 0.76404494 0.76404494
 0.76404494 0.76966292 0.76966292 0.76966292 0.76966292 0.76966292
 0.7752809  0.78089888 0.78089888 0.78089888 0.78651685 0.79213483
 0.79213483 0.79213483 0.79213483 0.79775281 0.79775281 0.79775281
 0.79775281 0.79775281 0.80337079 0.80898876 0.81460674 0.82022472
 0.82022472 0.82022472 0.8258427  0.8258427  0.83146067 0.83146067
 0.83146067 0.83146067 0.83146067 0.83707865 0.84269663 0.84831461
 0.84831461 0.84831461 0.84831461 0.84831461 0.84831461 0.84831461
 0.84831461 0.84831461 0.84831461 0.84831461 0.84831461 0.84831461
 0.85393258 0.85393258 0.85393258 0.85393258 0.85393258 0.85393258
 0.85955056 0.86516854 0.86516854 0.86516854 0.86516854 0.86516854
 0.87078652 0.87078652 0.87078652 0.87640449 0.87640449 0.87640449
 0.87640449 0.88202247 0.88202247 0.88764045 0.88764045 0.88764045
 0.88764045 0.88764045 0.88764045 0.89325843 0.89325843 0.89325843
 0.8988764  0.8988764  0.8988764  0.8988764  0.8988764  0.8988764
 0.90449438 0.91011236 0.91011236 0.91011236 0.91011236 0.91011236
 0.91573034 0.92134831 0.92696629 0.93258427 0.93820225 0.94382022
 0.94382022 0.9494382  0.9494382  0.95505618 0.95505618 0.95505618
 0.95505618 0.95505618 0.95505618 0.95505618 0.95505618 0.95505618
 0.95505618 0.95505618 0.96067416 0.96067416 0.96067416 0.96629213
 0.96629213 0.96629213 0.96629213 0.96629213 0.96629213 0.97191011
 0.97752809 0.97752809 0.98314607 0.98876404 0.98876404 0.99438202
 0.99438202 0.99438202 0.99438202 0.99438202 1.        ]
â„¹ INFO: Experiment EXPERIMENT8Reg10IoU - mAP: 0.7071
âœ… SUCCESS: Predictions saved to 'figures/'

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 23210390: <REG_10_CLF_1_iou> in cluster <dcc> Done

Job <REG_10_CLF_1_iou> was submitted from host <n-62-20-1> by user <s240466> in cluster <dcc> at Tue Nov 19 16:55:07 2024
Job was executed on host(s) <4*n-62-18-8>, in queue <c02516>, as user <s240466> in cluster <dcc> at Tue Nov 19 16:55:08 2024
</zhome/26/8/209207> was used as the home directory.
</zhome/26/8/209207/02516-intro-to-dl-in-cv/poster-3-object-detection> was used as the working directory.
Started at Tue Nov 19 16:55:08 2024
Terminated at Tue Nov 19 18:00:29 2024
Results reported at Tue Nov 19 18:00:29 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -q c02516
#BSUB -n 4
#BSUB -gpu "num=1:mode=exclusive_process" 
#BSUB -J REG_10_CLF_1_iou
#BSUB -R "span[hosts=1]"
#BSUB -R "rusage[mem=20GB]"
#BSUB -W 12:00
#BSUB -o job_outputs/REG_10_CLF_1_iou%J.out
#BSUB -e job_outputs/REG_10_CLF_1_iou%J.err

EXPERIMENT="EXPERIMENT8Reg10IoU"
EPOCHS=50
LEARNING_RATE=1e-4
IOU_THRESHOLD=0.1
CONFIDENCE_THRESHOLD=0.5
WEIGHT_DECAY=1e-5
NUM_IMAGES=10

source ~/venv/project2_venv/bin/activate
#conda activate project-3

python main.py \
    --experiment_name $EXPERIMENT \
    --num_images $NUM_IMAGES \
    --learning_rate $LEARNING_RATE \
    --num_epochs $EPOCHS \
    --iou_threshold $IOU_THRESHOLD \
    --confidence_threshold $CONFIDENCE_THRESHOLD \
    --weight_decay $WEIGHT_DECAY \
    --cls_weight 1.0 \
    --reg_weight 10.0
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   5103.00 sec.
    Max Memory :                                 5435 MB
    Average Memory :                             3486.48 MB
    Total Requested Memory :                     81920.00 MB
    Delta Memory :                               76485.00 MB
    Max Swap :                                   -
    Max Processes :                              10
    Max Threads :                                40
    Run time :                                   3924 sec.
    Turnaround time :                            3922 sec.

The output (if any) is above this job summary.



PS:

Read file <job_outputs/REG_10_CLF_1_iou23210390.err> for stderr output of this job.

