ðŸ”§ WORKING ON: Loading Train data
ðŸ”§ WORKING ON: Loading Validation data
ðŸ”§ WORKING ON: Loading Test data
â„¹ INFO: Using the entire dataset for training and validation.
ðŸ”§ WORKING ON: Training model
â„¹ INFO: Epoch 1/50 - Train Loss: 0.8095 (Cls: 0.3406, Reg: 0.4689) - Val Loss: 
0.4419 (Cls: 0.0986, Reg: 0.3433)
â„¹ INFO: Epoch 2/50 - Train Loss: 0.6348 (Cls: 0.2698, Reg: 0.3650) - Val Loss: 
0.4559 (Cls: 0.1159, Reg: 0.3400)
â„¹ INFO: Epoch 3/50 - Train Loss: 0.5866 (Cls: 0.2411, Reg: 0.3456) - Val Loss: 
0.4312 (Cls: 0.1066, Reg: 0.3246)
â„¹ INFO: Epoch 4/50 - Train Loss: 0.5528 (Cls: 0.2191, Reg: 0.3336) - Val Loss: 
0.4171 (Cls: 0.0983, Reg: 0.3188)
â„¹ INFO: Epoch 5/50 - Train Loss: 0.5259 (Cls: 0.2077, Reg: 0.3181) - Val Loss: 
0.4229 (Cls: 0.1198, Reg: 0.3031)
â„¹ INFO: Epoch 6/50 - Train Loss: 0.5003 (Cls: 0.1876, Reg: 0.3128) - Val Loss: 
0.3899 (Cls: 0.0939, Reg: 0.2960)
â„¹ INFO: Epoch 7/50 - Train Loss: 0.4781 (Cls: 0.1808, Reg: 0.2973) - Val Loss: 
0.4083 (Cls: 0.1064, Reg: 0.3019)
â„¹ INFO: Epoch 8/50 - Train Loss: 0.4479 (Cls: 0.1654, Reg: 0.2825) - Val Loss: 
0.3966 (Cls: 0.0967, Reg: 0.2999)
â„¹ INFO: Epoch 9/50 - Train Loss: 0.4367 (Cls: 0.1555, Reg: 0.2812) - Val Loss: 
0.3983 (Cls: 0.1030, Reg: 0.2953)
â„¹ INFO: Epoch 10/50 - Train Loss: 0.4139 (Cls: 0.1461, Reg: 0.2678) - Val Loss: 
0.4023 (Cls: 0.1081, Reg: 0.2941)
â„¹ INFO: Epoch 11/50 - Train Loss: 0.3954 (Cls: 0.1324, Reg: 0.2630) - Val Loss: 
0.4404 (Cls: 0.1434, Reg: 0.2970)
â„¹ INFO: Epoch 12/50 - Train Loss: 0.3726 (Cls: 0.1199, Reg: 0.2527) - Val Loss: 
0.4197 (Cls: 0.1287, Reg: 0.2910)
â„¹ INFO: Epoch 13/50 - Train Loss: 0.3605 (Cls: 0.1158, Reg: 0.2447) - Val Loss: 
0.3941 (Cls: 0.1077, Reg: 0.2865)
â„¹ INFO: Epoch 14/50 - Train Loss: 0.3408 (Cls: 0.1075, Reg: 0.2333) - Val Loss: 
0.4158 (Cls: 0.1168, Reg: 0.2990)
â„¹ INFO: Epoch 15/50 - Train Loss: 0.3302 (Cls: 0.0988, Reg: 0.2314) - Val Loss: 
0.4023 (Cls: 0.1125, Reg: 0.2898)
â„¹ INFO: Epoch 16/50 - Train Loss: 0.3144 (Cls: 0.0922, Reg: 0.2223) - Val Loss: 
0.4343 (Cls: 0.1416, Reg: 0.2927)
â„¹ INFO: Early stopping triggered.
ðŸ”§ WORKING ON: Visualizing predictions
Image ID: 565
Number of Proposals: 500
Predictions of potholes before NMS: 14
Predictions of potholes after NMS: 2
Image ID: 498
Number of Proposals: 500
Predictions of potholes before NMS: 2
Predictions of potholes after NMS: 1
Image ID: 11
Number of Proposals: 500
Predictions of potholes before NMS: 8
Predictions of potholes after NMS: 2
Image ID: 143
Number of Proposals: 500
Predictions of potholes before NMS: 3
Predictions of potholes after NMS: 1
Image ID: 65
Number of Proposals: 500
Predictions of potholes before NMS: 0
Predictions of potholes after NMS: 0
Image ID: 641
Number of Proposals: 500
Predictions of potholes before NMS: 0
Predictions of potholes after NMS: 0
Image ID: 627
Number of Proposals: 500
Predictions of potholes before NMS: 18
Predictions of potholes after NMS: 4
Image ID: 248
Number of Proposals: 500
Predictions of potholes before NMS: 7
Predictions of potholes after NMS: 4
Image ID: 611
Number of Proposals: 500
Predictions of potholes before NMS: 60
Predictions of potholes after NMS: 11
Image ID: 114
Number of Proposals: 500
Predictions of potholes before NMS: 10
Predictions of potholes after NMS: 3
Image Index: [96]
Number of Proposals: 8
Ground Truth Boxes: 2
Predictions Before NMS: 1
Predictions After NMS: 1
Image Index: [194]
Number of Proposals: 8
Ground Truth Boxes: 2
Predictions Before NMS: 0
Predictions After NMS: 0
Image Index: [354]
Number of Proposals: 8
Ground Truth Boxes: 2
Predictions Before NMS: 3
Predictions After NMS: 2
Image Index: [134]
Number of Proposals: 28
Ground Truth Boxes: 7
Predictions Before NMS: 4
Predictions After NMS: 1
Image Index: [309]
Number of Proposals: 32
Ground Truth Boxes: 8
Predictions Before NMS: 1
Predictions After NMS: 1
ðŸ”§ WORKING ON: Saving model
â„¹ INFO: Model state_dict saved to 
saved_models/model_state_dict_EXPERIMENT6Reg10lowIoU.pth
â„¹ INFO: Entire model saved to saved_models/model_full_EXPERIMENT6Reg10lowIoU.pth
ðŸ”§ WORKING ON: Evaluating model on Validation split
â„¹ INFO: Precision-Recall curve saved as 
figures/png/EXPERIMENT6Reg10lowIoU/precision_recall_curve_val_EXPERIMENT6Reg10lo
wIoU_20241119-172700.png and 
figures/svg/EXPERIMENT6Reg10lowIoU/precision_recall_curve_val_EXPERIMENT6Reg10lo
wIoU_20241119-172700.svg
precision: [1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.         1.
 1.         1.         0.96296296 0.96428571 0.96551724 0.96666667
 0.96774194 0.96875    0.96969697 0.97058824 0.97142857 0.97222222
 0.97297297 0.97368421 0.97435897 0.975      0.97560976 0.97619048
 0.97674419 0.97727273 0.95555556 0.95652174 0.95744681 0.95833333
 0.93877551 0.92       0.90196078 0.90384615 0.90566038 0.90740741
 0.90909091 0.89285714 0.89473684 0.89655172 0.89830508 0.88333333
 0.86885246 0.87096774 0.87301587 0.875      0.87692308 0.87878788
 0.88059701 0.86764706 0.86956522 0.85714286 0.85915493 0.84722222
 0.83561644 0.83783784 0.84       0.82894737 0.83116883 0.83333333
 0.83544304 0.8375     0.83950617 0.82926829 0.81927711 0.82142857
 0.82352941 0.8255814  0.82758621 0.82954545 0.82022472 0.81111111
 0.81318681 0.80434783 0.80645161 0.79787234 0.8        0.79166667
 0.78350515 0.78571429 0.78787879 0.79       0.78217822 0.7745098
 0.76699029 0.76923077 0.77142857 0.76415094 0.75700935 0.75925926
 0.76146789 0.76363636 0.75675676 0.75       0.74336283 0.74561404
 0.74782609 0.74137931 0.74358974 0.73728814 0.73109244 0.73333333
 0.73553719 0.7295082  0.73170732 0.72580645 0.728      0.72222222
 0.71653543 0.71875    0.72093023 0.72307692 0.71755725 0.71212121
 0.70676692 0.70895522 0.7037037  0.70588235 0.70072993 0.70289855
 0.69784173 0.7        0.69503546 0.6971831  0.69230769 0.6875
 0.68965517 0.68493151 0.68027211 0.68243243 0.67785235 0.67333333
 0.66887417 0.67105263 0.66666667 0.66883117 0.66451613 0.66666667
 0.66242038 0.66455696 0.66037736 0.65625    0.65838509 0.66049383
 0.65644172 0.65853659 0.66060606 0.6626506  0.65868263 0.6547619
 0.65088757 0.64705882 0.64327485 0.64534884 0.64739884 0.64367816
 0.64       0.64204545 0.6440678  0.64606742 0.6424581  0.64444444
 0.64640884 0.64835165 0.65027322 0.65217391 0.64864865 0.65053763
 0.64705882 0.64893617 0.64550265 0.64210526 0.63874346 0.640625
 0.6373057  0.63402062 0.63076923 0.62755102 0.62944162 0.62626263
 0.6281407  0.625      0.62189055 0.61881188 0.62068966 0.61764706
 0.61463415 0.61165049 0.61352657 0.61057692 0.61244019 0.60952381
 0.60663507 0.60377358 0.6056338  0.60280374 0.6        0.60185185
 0.59907834 0.60091743 0.59817352 0.6        0.59728507 0.5990991
 0.59641256 0.59375    0.59555556 0.59292035 0.59030837 0.5877193
 0.58951965 0.59130435 0.59307359 0.59482759 0.59227468 0.59401709
 0.59148936 0.58898305 0.58649789 0.58403361 0.58577406 0.58333333
 0.58506224 0.58677686 0.58436214 0.58196721 0.57959184 0.58130081
 0.57894737 0.58064516 0.58232932 0.58       0.57768924 0.57539683
 0.57312253 0.57086614 0.56862745 0.56640625 0.56420233 0.5620155
 0.56370656 0.56538462 0.56321839 0.5648855  0.56653992 0.56439394
 0.56603774 0.56390977 0.56554307 0.56716418 0.56505576 0.56296296
 0.56457565 0.56617647 0.56410256 0.56569343 0.56727273 0.56884058
 0.566787   0.5647482  0.56272401 0.56071429 0.56227758 0.56382979
 0.56183746 0.56338028 0.56491228 0.56643357 0.56794425 0.56944444
 0.56747405 0.56551724 0.56701031 0.56506849 0.56313993 0.56122449
 0.55932203 0.56081081 0.55892256 0.56040268 0.55852843 0.55666667
 0.55813953 0.55960265 0.56105611 0.55921053 0.55737705 0.55555556
 0.55374593 0.55519481 0.55339806 0.5516129  0.54983923 0.55128205
 0.54952077 0.5477707  0.54603175 0.5443038  0.54258675 0.5408805
 0.53918495 0.5375     0.53582555 0.53416149 0.53250774 0.5308642
 0.53230769 0.53067485 0.53211009 0.5304878  0.53191489 0.53333333
 0.5347432  0.53313253 0.53153153 0.53293413 0.53134328 0.5297619
 0.52818991 0.52662722 0.5280236  0.52647059 0.52492669 0.52339181
 0.52186589 0.52034884 0.51884058 0.51734104 0.51585014 0.51724138
 0.51575931 0.51428571 0.51282051 0.51136364 0.50991501 0.50847458
 0.50985915 0.51123596 0.50980392 0.51117318 0.51253482 0.51111111
 0.50969529 0.50828729 0.50964187 0.51098901 0.51232877 0.5136612
 0.51498638 0.51358696 0.51219512 0.51351351 0.5148248  0.51612903
 0.51474531 0.51604278 0.51466667 0.51329787 0.51193634 0.51058201
 0.50923483 0.50789474 0.50656168 0.5078534  0.50652742 0.50520833
 0.5038961  0.50259067 0.50129199 0.50257732 0.50128535 0.5025641
 0.50127877 0.5        0.50127226 0.5        0.49873418 0.5
 0.49874055 0.49748744 0.49874687 0.4975     0.49625935 0.49751244
 0.49875931 0.49752475 0.4962963  0.49753695 0.4963145  0.49509804
 0.49388753 0.49268293 0.49148418 0.49029126 0.48910412 0.48792271
 0.48915663 0.48798077 0.48920863 0.48803828 0.48926014 0.49047619
 0.48931116 0.48815166 0.4893617  0.48820755 0.48705882 0.48591549
 0.48477752 0.48364486 0.48251748 0.48139535 0.48259861 0.48148148
 0.48267898 0.48156682]
recall: [0.00478469 0.00956938 0.01435407 0.01913876 0.02392344 0.02870813
 0.03349282 0.03827751 0.0430622  0.04784689 0.05263158 0.05741627
 0.06220096 0.06698565 0.07177033 0.07655502 0.08133971 0.0861244
 0.09090909 0.09569378 0.10047847 0.10526316 0.11004785 0.11483254
 0.11961722 0.12440191 0.12440191 0.1291866  0.13397129 0.13875598
 0.14354067 0.14832536 0.15311005 0.15789474 0.16267943 0.16746411
 0.1722488  0.17703349 0.18181818 0.18660287 0.19138756 0.19617225
 0.20095694 0.20574163 0.20574163 0.21052632 0.215311   0.22009569
 0.22009569 0.22009569 0.22009569 0.22488038 0.22966507 0.23444976
 0.23923445 0.23923445 0.24401914 0.24880383 0.25358852 0.25358852
 0.25358852 0.25837321 0.26315789 0.26794258 0.27272727 0.27751196
 0.28229665 0.28229665 0.28708134 0.28708134 0.29186603 0.29186603
 0.29186603 0.29665072 0.30143541 0.30143541 0.3062201  0.31100478
 0.31578947 0.32057416 0.32535885 0.32535885 0.32535885 0.33014354
 0.33492823 0.33971292 0.34449761 0.3492823  0.3492823  0.3492823
 0.35406699 0.35406699 0.35885167 0.35885167 0.36363636 0.36363636
 0.36363636 0.36842105 0.37320574 0.37799043 0.37799043 0.37799043
 0.37799043 0.38277512 0.38755981 0.38755981 0.38755981 0.3923445
 0.39712919 0.40191388 0.40191388 0.40191388 0.40191388 0.40669856
 0.41148325 0.41148325 0.41626794 0.41626794 0.41626794 0.42105263
 0.42583732 0.42583732 0.43062201 0.43062201 0.4354067  0.4354067
 0.4354067  0.44019139 0.44497608 0.44976077 0.44976077 0.44976077
 0.44976077 0.45454545 0.45454545 0.45933014 0.45933014 0.46411483
 0.46411483 0.46889952 0.46889952 0.47368421 0.47368421 0.47368421
 0.4784689  0.4784689  0.4784689  0.48325359 0.48325359 0.48325359
 0.48325359 0.48803828 0.48803828 0.49282297 0.49282297 0.49760766
 0.49760766 0.50239234 0.50239234 0.50239234 0.50717703 0.51196172
 0.51196172 0.51674641 0.5215311  0.52631579 0.52631579 0.52631579
 0.52631579 0.52631579 0.52631579 0.53110048 0.53588517 0.53588517
 0.53588517 0.54066986 0.54545455 0.55023923 0.55023923 0.55502392
 0.55980861 0.5645933  0.56937799 0.57416268 0.57416268 0.57894737
 0.57894737 0.58373206 0.58373206 0.58373206 0.58373206 0.58851675
 0.58851675 0.58851675 0.58851675 0.58851675 0.59330144 0.59330144
 0.59808612 0.59808612 0.59808612 0.59808612 0.60287081 0.60287081
 0.60287081 0.60287081 0.6076555  0.6076555  0.61244019 0.61244019
 0.61244019 0.61244019 0.61722488 0.61722488 0.61722488 0.62200957
 0.62200957 0.62679426 0.62679426 0.63157895 0.63157895 0.63636364
 0.63636364 0.63636364 0.64114833 0.64114833 0.64114833 0.64114833
 0.64593301 0.6507177  0.65550239 0.66028708 0.66028708 0.66507177
 0.66507177 0.66507177 0.66507177 0.66507177 0.66985646 0.66985646
 0.67464115 0.67942584 0.67942584 0.67942584 0.67942584 0.68421053
 0.68421053 0.68899522 0.6937799  0.6937799  0.6937799  0.6937799
 0.6937799  0.6937799  0.6937799  0.6937799  0.6937799  0.6937799
 0.69856459 0.70334928 0.70334928 0.70813397 0.71291866 0.71291866
 0.71770335 0.71770335 0.72248804 0.72727273 0.72727273 0.72727273
 0.73205742 0.73684211 0.73684211 0.74162679 0.74641148 0.75119617
 0.75119617 0.75119617 0.75119617 0.75119617 0.75598086 0.76076555
 0.76076555 0.76555024 0.77033493 0.77511962 0.77990431 0.784689
 0.784689   0.784689   0.78947368 0.78947368 0.78947368 0.78947368
 0.78947368 0.79425837 0.79425837 0.79904306 0.79904306 0.79904306
 0.80382775 0.80861244 0.81339713 0.81339713 0.81339713 0.81339713
 0.81339713 0.81818182 0.81818182 0.81818182 0.81818182 0.82296651
 0.82296651 0.82296651 0.82296651 0.82296651 0.82296651 0.82296651
 0.82296651 0.82296651 0.82296651 0.82296651 0.82296651 0.82296651
 0.8277512  0.8277512  0.83253589 0.83253589 0.83732057 0.84210526
 0.84688995 0.84688995 0.84688995 0.85167464 0.85167464 0.85167464
 0.85167464 0.85167464 0.85645933 0.85645933 0.85645933 0.85645933
 0.85645933 0.85645933 0.85645933 0.85645933 0.85645933 0.86124402
 0.86124402 0.86124402 0.86124402 0.86124402 0.86124402 0.86124402
 0.86602871 0.8708134  0.8708134  0.87559809 0.88038278 0.88038278
 0.88038278 0.88038278 0.88516746 0.88995215 0.89473684 0.89952153
 0.90430622 0.90430622 0.90430622 0.90909091 0.9138756  0.91866029
 0.91866029 0.92344498 0.92344498 0.92344498 0.92344498 0.92344498
 0.92344498 0.92344498 0.92344498 0.92822967 0.92822967 0.92822967
 0.92822967 0.92822967 0.92822967 0.93301435 0.93301435 0.93779904
 0.93779904 0.93779904 0.94258373 0.94258373 0.94258373 0.94736842
 0.94736842 0.94736842 0.95215311 0.95215311 0.95215311 0.9569378
 0.96172249 0.96172249 0.96172249 0.96650718 0.96650718 0.96650718
 0.96650718 0.96650718 0.96650718 0.96650718 0.96650718 0.96650718
 0.97129187 0.97129187 0.97607656 0.97607656 0.98086124 0.98564593
 0.98564593 0.98564593 0.99043062 0.99043062 0.99043062 0.99043062
 0.99043062 0.99043062 0.99043062 0.99043062 0.99521531 0.99521531
 1.         1.        ]
â„¹ INFO: Experiment EXPERIMENT6Reg10lowIoU - mAP: 0.7250
ðŸ”§ WORKING ON: Evaluating model on Test split
â„¹ INFO: Precision-Recall curve saved as 
figures/png/EXPERIMENT6Reg10lowIoU/precision_recall_curve_test_EXPERIMENT6Reg10l
owIoU_20241119-172910.png and 
figures/svg/EXPERIMENT6Reg10lowIoU/precision_recall_curve_test_EXPERIMENT6Reg10l
owIoU_20241119-172910.svg
precision: [1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         0.90909091 0.91666667
 0.92307692 0.92857143 0.86666667 0.875      0.88235294 0.88888889
 0.89473684 0.9        0.9047619  0.90909091 0.91304348 0.91666667
 0.92       0.88461538 0.88888889 0.89285714 0.89655172 0.9
 0.90322581 0.90625    0.90909091 0.91176471 0.91428571 0.91666667
 0.91891892 0.92105263 0.92307692 0.925      0.90243902 0.9047619
 0.90697674 0.88636364 0.88888889 0.89130435 0.89361702 0.89583333
 0.89795918 0.9        0.88235294 0.88461538 0.88679245 0.87037037
 0.87272727 0.875      0.87719298 0.87931034 0.88135593 0.88333333
 0.8852459  0.88709677 0.88888889 0.875      0.87692308 0.87878788
 0.88059701 0.88235294 0.88405797 0.88571429 0.87323944 0.875
 0.87671233 0.87837838 0.88       0.88157895 0.88311688 0.87179487
 0.86075949 0.8625     0.85185185 0.85365854 0.85542169 0.85714286
 0.85882353 0.86046512 0.86206897 0.86363636 0.86516854 0.85555556
 0.85714286 0.84782609 0.84946237 0.84042553 0.84210526 0.84375
 0.84536082 0.84693878 0.83838384 0.83       0.83168317 0.83333333
 0.83495146 0.83653846 0.83809524 0.83962264 0.8411215  0.83333333
 0.82568807 0.81818182 0.81981982 0.82142857 0.82300885 0.8245614
 0.82608696 0.82758621 0.82905983 0.8220339  0.81512605 0.81666667
 0.81818182 0.81147541 0.81300813 0.81451613 0.816      0.81746032
 0.81889764 0.8203125  0.81395349 0.80769231 0.80152672 0.79545455
 0.79699248 0.79850746 0.79259259 0.78676471 0.78832117 0.7826087
 0.78417266 0.78571429 0.78723404 0.78169014 0.78321678 0.77777778
 0.77931034 0.78082192 0.78231293 0.77702703 0.77852349 0.77333333
 0.76821192 0.76973684 0.77124183 0.77272727 0.76774194 0.76923077
 0.76433121 0.75949367 0.75471698 0.75625    0.75776398 0.75925926
 0.75460123 0.75       0.74545455 0.74698795 0.74251497 0.73809524
 0.73372781 0.72941176 0.73099415 0.73255814 0.73410405 0.73563218
 0.73142857 0.72727273 0.72881356 0.73033708 0.73184358 0.72777778
 0.72375691 0.72527473 0.72131148 0.7173913  0.71891892 0.71505376
 0.71122995 0.70744681 0.7037037  0.7        0.69633508 0.69270833
 0.68911917 0.68556701 0.68717949 0.68877551 0.69035533 0.68686869
 0.68844221 0.685      0.68159204 0.67821782 0.67980296 0.68137255
 0.68292683 0.67961165 0.6763285  0.67307692 0.67464115 0.67619048
 0.67772512 0.67924528 0.67605634 0.6728972  0.66976744 0.66666667
 0.66359447 0.66055046 0.65753425 0.65454545 0.65158371 0.64864865
 0.65022422 0.64732143 0.64444444 0.64159292 0.63876652 0.63596491
 0.63318777 0.63478261 0.63636364 0.63362069 0.63090129 0.62820513
 0.62553191 0.62711864 0.62447257 0.62605042 0.62761506 0.625
 0.62240664 0.61983471 0.62139918 0.62295082 0.6244898  0.62601626
 0.62753036 0.625      0.62248996 0.62       0.62151394 0.62301587
 0.62055336 0.62204724 0.62352941 0.625      0.62645914 0.62403101
 0.62548263 0.62307692 0.62068966 0.61832061 0.61977186 0.61742424
 0.61509434 0.61278195 0.61048689 0.60820896 0.60966543 0.60740741
 0.60516605 0.60294118 0.6007326  0.60218978 0.6        0.60144928
 0.59927798 0.60071942 0.59856631 0.6        0.59786477 0.59929078
 0.60070671 0.60211268 0.60350877 0.6013986  0.59930314 0.59722222
 0.59515571 0.59310345 0.59106529 0.5890411  0.59044369 0.59183673
 0.59322034 0.59459459 0.59259259 0.59060403 0.58862876 0.58666667
 0.58471761 0.58278146 0.58415842 0.58223684 0.58032787 0.57843137
 0.57654723 0.57467532 0.57281553 0.57096774 0.57234727 0.57051282
 0.5686901  0.56687898 0.56507937 0.56329114 0.5615142  0.55974843
 0.55799373 0.559375   0.5576324  0.55590062 0.55417957 0.55246914
 0.55076923 0.54907975 0.55045872 0.54878049 0.54711246 0.54545455
 0.54380665 0.54216867 0.54054054 0.53892216 0.53731343 0.53571429
 0.53412463 0.53254438 0.53097345 0.53235294 0.53372434 0.53508772
 0.5335277  0.53197674 0.53333333 0.53179191 0.53025937 0.52873563
 0.52722063 0.52857143 0.52706553 0.52840909 0.52691218 0.52542373
 0.52394366 0.52247191 0.5210084  0.51955307 0.51810585 0.51666667
 0.51523546 0.51657459 0.51515152 0.51373626 0.51232877 0.51092896
 0.50953678 0.50815217 0.50948509 0.51081081 0.50943396 0.51075269
 0.50938338 0.50802139 0.50933333 0.50797872 0.5066313  0.50529101
 0.50395778 0.50263158 0.50393701 0.5052356  0.50391645 0.50260417
 0.5012987  0.50259067 0.50129199 0.5        0.49871465 0.5
 0.50127877 0.50255102 0.50127226 0.5        0.49873418 0.49747475
 0.49622166 0.49748744 0.4962406  0.495      0.49376559 0.49502488
 0.49379653 0.4950495  0.49382716 0.49261084 0.49140049 0.49264706
 0.49144254 0.4902439  0.48905109 0.48786408 0.48910412 0.49033816
 0.49156627 0.49038462 0.49160671 0.49043062 0.48926014 0.48809524
 0.48693587 0.48815166 0.48699764 0.48584906 0.48470588 0.48356808
 0.48477752 0.48598131 0.48484848 0.48604651 0.48491879 0.4837963
 0.48267898 0.48387097 0.48275862 0.48165138 0.4805492  0.47945205
 0.47835991 0.47727273 0.47845805 0.47963801 0.4785553  0.47747748
 0.47640449 0.47757848 0.47651007 0.47544643 0.47438753 0.47333333
 0.47450111 0.47345133 0.47240618 0.47356828 0.47252747 0.47368421
 0.4726477  0.47161572 0.47058824 0.46956522 0.46854664 0.46753247
 0.46652268 0.46551724 0.46451613 0.46351931 0.46252677]
recall: [0.00462963 0.00925926 0.01388889 0.01851852 0.02314815 0.02777778
 0.03240741 0.03703704 0.04166667 0.0462963  0.0462963  0.05092593
 0.05555556 0.06018519 0.06018519 0.06481481 0.06944444 0.07407407
 0.0787037  0.08333333 0.08796296 0.09259259 0.09722222 0.10185185
 0.10648148 0.10648148 0.11111111 0.11574074 0.12037037 0.125
 0.12962963 0.13425926 0.13888889 0.14351852 0.14814815 0.15277778
 0.15740741 0.16203704 0.16666667 0.1712963  0.1712963  0.17592593
 0.18055556 0.18055556 0.18518519 0.18981481 0.19444444 0.19907407
 0.2037037  0.20833333 0.20833333 0.21296296 0.21759259 0.21759259
 0.22222222 0.22685185 0.23148148 0.23611111 0.24074074 0.24537037
 0.25       0.25462963 0.25925926 0.25925926 0.26388889 0.26851852
 0.27314815 0.27777778 0.28240741 0.28703704 0.28703704 0.29166667
 0.2962963  0.30092593 0.30555556 0.31018519 0.31481481 0.31481481
 0.31481481 0.31944444 0.31944444 0.32407407 0.3287037  0.33333333
 0.33796296 0.34259259 0.34722222 0.35185185 0.35648148 0.35648148
 0.36111111 0.36111111 0.36574074 0.36574074 0.37037037 0.375
 0.37962963 0.38425926 0.38425926 0.38425926 0.38888889 0.39351852
 0.39814815 0.40277778 0.40740741 0.41203704 0.41666667 0.41666667
 0.41666667 0.41666667 0.4212963  0.42592593 0.43055556 0.43518519
 0.43981481 0.44444444 0.44907407 0.44907407 0.44907407 0.4537037
 0.45833333 0.45833333 0.46296296 0.46759259 0.47222222 0.47685185
 0.48148148 0.48611111 0.48611111 0.48611111 0.48611111 0.48611111
 0.49074074 0.49537037 0.49537037 0.49537037 0.5        0.5
 0.50462963 0.50925926 0.51388889 0.51388889 0.51851852 0.51851852
 0.52314815 0.52777778 0.53240741 0.53240741 0.53703704 0.53703704
 0.53703704 0.54166667 0.5462963  0.55092593 0.55092593 0.55555556
 0.55555556 0.55555556 0.55555556 0.56018519 0.56481481 0.56944444
 0.56944444 0.56944444 0.56944444 0.57407407 0.57407407 0.57407407
 0.57407407 0.57407407 0.5787037  0.58333333 0.58796296 0.59259259
 0.59259259 0.59259259 0.59722222 0.60185185 0.60648148 0.60648148
 0.60648148 0.61111111 0.61111111 0.61111111 0.61574074 0.61574074
 0.61574074 0.61574074 0.61574074 0.61574074 0.61574074 0.61574074
 0.61574074 0.61574074 0.62037037 0.625      0.62962963 0.62962963
 0.63425926 0.63425926 0.63425926 0.63425926 0.63888889 0.64351852
 0.64814815 0.64814815 0.64814815 0.64814815 0.65277778 0.65740741
 0.66203704 0.66666667 0.66666667 0.66666667 0.66666667 0.66666667
 0.66666667 0.66666667 0.66666667 0.66666667 0.66666667 0.66666667
 0.6712963  0.6712963  0.6712963  0.6712963  0.6712963  0.6712963
 0.6712963  0.67592593 0.68055556 0.68055556 0.68055556 0.68055556
 0.68055556 0.68518519 0.68518519 0.68981481 0.69444444 0.69444444
 0.69444444 0.69444444 0.69907407 0.7037037  0.70833333 0.71296296
 0.71759259 0.71759259 0.71759259 0.71759259 0.72222222 0.72685185
 0.72685185 0.73148148 0.73611111 0.74074074 0.74537037 0.74537037
 0.75       0.75       0.75       0.75       0.75462963 0.75462963
 0.75462963 0.75462963 0.75462963 0.75462963 0.75925926 0.75925926
 0.75925926 0.75925926 0.75925926 0.76388889 0.76388889 0.76851852
 0.76851852 0.77314815 0.77314815 0.77777778 0.77777778 0.78240741
 0.78703704 0.79166667 0.7962963  0.7962963  0.7962963  0.7962963
 0.7962963  0.7962963  0.7962963  0.7962963  0.80092593 0.80555556
 0.81018519 0.81481481 0.81481481 0.81481481 0.81481481 0.81481481
 0.81481481 0.81481481 0.81944444 0.81944444 0.81944444 0.81944444
 0.81944444 0.81944444 0.81944444 0.81944444 0.82407407 0.82407407
 0.82407407 0.82407407 0.82407407 0.82407407 0.82407407 0.82407407
 0.82407407 0.8287037  0.8287037  0.8287037  0.8287037  0.8287037
 0.8287037  0.8287037  0.83333333 0.83333333 0.83333333 0.83333333
 0.83333333 0.83333333 0.83333333 0.83333333 0.83333333 0.83333333
 0.83333333 0.83333333 0.83333333 0.83796296 0.84259259 0.84722222
 0.84722222 0.84722222 0.85185185 0.85185185 0.85185185 0.85185185
 0.85185185 0.85648148 0.85648148 0.86111111 0.86111111 0.86111111
 0.86111111 0.86111111 0.86111111 0.86111111 0.86111111 0.86111111
 0.86111111 0.86574074 0.86574074 0.86574074 0.86574074 0.86574074
 0.86574074 0.86574074 0.87037037 0.875      0.875      0.87962963
 0.87962963 0.87962963 0.88425926 0.88425926 0.88425926 0.88425926
 0.88425926 0.88425926 0.88888889 0.89351852 0.89351852 0.89351852
 0.89351852 0.89814815 0.89814815 0.89814815 0.89814815 0.90277778
 0.90740741 0.91203704 0.91203704 0.91203704 0.91203704 0.91203704
 0.91203704 0.91666667 0.91666667 0.91666667 0.91666667 0.9212963
 0.9212963  0.92592593 0.92592593 0.92592593 0.92592593 0.93055556
 0.93055556 0.93055556 0.93055556 0.93055556 0.93518519 0.93981481
 0.94444444 0.94444444 0.94907407 0.94907407 0.94907407 0.94907407
 0.94907407 0.9537037  0.9537037  0.9537037  0.9537037  0.9537037
 0.95833333 0.96296296 0.96296296 0.96759259 0.96759259 0.96759259
 0.96759259 0.97222222 0.97222222 0.97222222 0.97222222 0.97222222
 0.97222222 0.97222222 0.97685185 0.98148148 0.98148148 0.98148148
 0.98148148 0.98611111 0.98611111 0.98611111 0.98611111 0.98611111
 0.99074074 0.99074074 0.99074074 0.99537037 0.99537037 1.
 1.         1.         1.         1.         1.         1.
 1.         1.         1.         1.         1.        ]
â„¹ INFO: Experiment EXPERIMENT6Reg10lowIoU - mAP: 0.7530
âœ… SUCCESS: Predictions saved to 'figures/'

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 23210340: <REG_10_CLF_1_iou> in cluster <dcc> Done

Job <REG_10_CLF_1_iou> was submitted from host <n-62-12-20> by user <s233498> in cluster <dcc> at Tue Nov 19 16:38:02 2024
Job was executed on host(s) <4*n-62-18-8>, in queue <c02516>, as user <s233498> in cluster <dcc> at Tue Nov 19 16:38:04 2024
</zhome/e3/f/203690> was used as the home directory.
</dtu/blackhole/0a/203690/02516-intro-to-dl-in-cv/poster-3-object-detection> was used as the working directory.
Started at Tue Nov 19 16:38:04 2024
Terminated at Tue Nov 19 17:29:12 2024
Results reported at Tue Nov 19 17:29:12 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -q c02516
#BSUB -n 4
#BSUB -gpu "num=1:mode=exclusive_process" 
#BSUB -J REG_10_CLF_1_iou
#BSUB -R "span[hosts=1]"
#BSUB -R "rusage[mem=20GB]"
#BSUB -W 12:00
#BSUB -o job_outputs/REG_10_CLF_1_iou%J.out
#BSUB -e job_outputs/REG_10_CLF_1_iou%J.err

EXPERIMENT="EXPERIMENT6Reg10lowIoU"
EPOCHS=50
LEARNING_RATE=1e-4
IOU_THRESHOLD=0.05
CONFIDENCE_THRESHOLD=0.5
WEIGHT_DECAY=1e-5
NUM_IMAGES=10

source /dtu/blackhole/0a/203690/miniconda3/bin/activate

conda activate project-3

python main.py \
    --experiment_name $EXPERIMENT \
    --num_images $NUM_IMAGES \
    --learning_rate $LEARNING_RATE \
    --num_epochs $EPOCHS \
    --iou_threshold $IOU_THRESHOLD \
    --confidence_threshold $CONFIDENCE_THRESHOLD \
    --weight_decay $WEIGHT_DECAY \
    --cls_weight 1.0 \
    --reg_weight 10.0
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   3999.74 sec.
    Max Memory :                                 5253 MB
    Average Memory :                             3497.41 MB
    Total Requested Memory :                     81920.00 MB
    Delta Memory :                               76667.00 MB
    Max Swap :                                   -
    Max Processes :                              10
    Max Threads :                                94
    Run time :                                   3073 sec.
    Turnaround time :                            3070 sec.

The output (if any) is above this job summary.



PS:

Read file <job_outputs/REG_10_CLF_1_iou23210340.err> for stderr output of this job.

